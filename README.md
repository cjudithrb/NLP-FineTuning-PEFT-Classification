# Fine-tuning Classification - Espa√±ol

Este notebook implementa un pipeline completo de fine-tuning para clasificaci√≥n de textos en espa√±ol, espec√≠ficamente para detectar intenci√≥n suicida en textos. Utiliza t√©cnicas modernas como LoRA (Low-Rank Adaptation) y la plataforma Hugging Face.

## üìã Descripci√≥n General

El notebook `Fine_tuning_Clasification.ipynb` realiza las siguientes tareas:

1. **Pre-requisitos**: Instalaci√≥n de dependencias necesarias
2. **Configuraci√≥n**: Setup de modelos, dispositivos (GPU/CPU) y variables de entorno
3. **Carga de Dataset**: Obtiene el dataset de Hugging Face
4. **Tokenizaci√≥n**: Preprocesamiento de textos
5. **Configuraci√≥n LoRA**: Ajuste de par√°metros para fine-tuning eficiente
6. **Entrenamiento**: Training del modelo con callbacks y monitoreo
7. **Evaluaci√≥n**: Comparaci√≥n de rendimiento base vs fine-tuned
8. **Guardado**: Exportaci√≥n del modelo a Hugging Face Hub

## üöÄ Requisitos Previos

- Python 3.8+
- CUDA (opcional, para GPU)
- Cuenta en Hugging Face Hub
- Token de Hugging Face

## üì¶ Dependencias Principales

```
transformers
accelerate
datasets
peft
torch
wandb
evaluate
rouge-score
pandas
numpy
scikit-learn
```

## üõ†Ô∏è Instalaci√≥n

Las dependencias se instalan autom√°ticamente en las primeras celdas del notebook:

```python
!pip install transformers
!pip install accelerate
!pip install wandb
!pip install evaluate==0.4.0 rouge-score==0.1.2
!pip install peft==0.13.2 datasets==3.0.1 wandb==0.13.1
```

## üìä Dataset

El notebook utiliza el dataset `spanish-suicide-intent` disponible en Hugging Face Hub:
- **Estructura**: Train, Validation y Test splits
- **Tarea**: Clasificaci√≥n binaria (intenci√≥n suicida: S√≠/No)
- **Idioma**: Espa√±ol

```python
dataset = load_dataset("PrevenIA/spanish-suicide-intent")
```

## ü§ñ Modelos Soportados

El notebook est√° configurado para trabajar con diferentes modelos base:
- `Qwen/Qwen2.5-0.5B` (por defecto)
- `Qwen/Qwen3-0.6B`
- `dccuchile/bert-base-spanish-wwm-cased`
- `UMUTeam/emotions-DistilBETO`
- `meta-llama/Llama-3.2-1B`

## ‚öôÔ∏è Par√°metros de Configuraci√≥n

### Configuraci√≥n LoRA
```python
LORA_RANK = 4
LORA_ALPHA = 8
LORA_DROPOUT = 0.05
TASK_TYPE = TaskType.SEQ_CLS
```

### Configuraci√≥n de Entrenamiento
```python
num_train_epochs=2
learning_rate=0.001
per_device_train_batch_size=16
gradient_accumulation_steps=16
eval_steps=4
fp16=True  # Mixed precision training
```

## üîÑ Flujo del Notebook

### 1. Pre-requisitos (Secci√≥n 1)
- Instalaci√≥n de dependencias
- Importaci√≥n de librer√≠as necesarias

### 2. Configuraci√≥n (Secci√≥n 2)
- Variables globales y nombres de modelos
- Autenticaci√≥n con Hugging Face Hub
- Carga del modelo y tokenizador

### 3. Dataset (Secci√≥n 3)
- Carga del dataset desde HF Hub
- Exploraci√≥n de datos
- Pruebas de inferencia con el modelo base

### 4. Entrenamiento (Secci√≥n 4)
- Tokenizaci√≥n y preparaci√≥n de datos
- Configuraci√≥n de LoRA
- Training con monitoreo en W&B
- Callbacks para early stopping

### 5. Evaluaci√≥n (Secci√≥n 5)
- Comparaci√≥n modelo base vs PEFT
- M√©tricas: Accuracy, F1, Precision, Recall
- An√°lisis caso por caso

### 6. Guardado (Secci√≥n Final)
- Guardado local del modelo
- Carga de credenciales HF
- Upload a Hugging Face Hub

## üìà Monitoreo y Logging

El notebook integra **Weights & Biases (W&B)** para monitoreo:

```python
PROJECT_NAME_WANDB = "FinetuningClasificacion"
wandb.init(project=PROJECT_NAME_WANDB, name=f"Run_{model_name}_{current_datetime}")
```

Se registran las siguientes m√©tricas:
- Loss (entrenamiento y validaci√≥n)
- Accuracy
- F1, Precision, Recall
- Learning rate
- Epoch y steps

## üíæ Guardado y Carga de Modelos

### Guardar en Hugging Face Hub
```python
save_model_locally(peft_model, REPO_LOCAL_NAME)
create_or_get_repo(REPO_HF_NAME, HF_TOKEN)
upload_lora_adapters(REPO_HF_NAME, REPO_LOCAL_NAME, HF_TOKEN)
```

### Cargar desde Hub
```python
from peft import PeftConfig, PeftModel
config = PeftConfig.from_pretrained(REPO_HF_NAME)
optimized_model = PeftModel.from_pretrained(base_model, REPO_HF_NAME)
```

## üîë Variables de Entorno

```bash
HUGGING_FACE_HUB_TOKEN=<your_token>
HF_HUB_ENABLE_HF_TRANSFER=1
```

## üìù Funciones Principales

### `load_model_and_tokenizer(base_model_name)`
Carga el modelo y tokenizador preentrenados.

### `tokenize_preprocess(examples, tokenizer, max_length=64)`
Tokeniza los textos y prepara las etiquetas.

### `prepare_datasets(dataset, tokenizer, test_size=0.1)`
Divide el dataset en train, validation y test.

### `configure_lora(...)`
Configura los par√°metros de LoRA.

### `compute_metrics(eval_pred)`
Calcula m√©tricas de evaluaci√≥n.

### `train_model(...)`
Ejecuta el training con callbacks.

## üéØ Resultados Esperados

El modelo fine-tuned con LoRA t√≠picamente:
- Reduce significativamente los par√°metros entrenables
- Mejora el accuracy respecto al modelo base
- Mantiene o mejora F1, Precision y Recall
- Requiere menos memoria y tiempo de entrenamiento

## üêõ Troubleshooting

### Error de CUDA/Memoria
```python
import torch
torch.cuda.empty_cache()
```

O ajustar en variables de entorno:
```bash
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### Token de HF no encontrado
Ejecutar la celda de login y proporcionar el token cuando se solicite.

### Problemas con Pad Token
El notebook maneja autom√°ticamente la configuraci√≥n del pad_token para diferentes tokenizadores.

## üìö Referencias

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
- [Weights & Biases](https://wandb.ai/)

## üë§ Autor

Judith

## üìÑ Licencia

Este proyecto utiliza datasets y modelos disponibles bajo licencias de Hugging Face.

---

**√öltima actualizaci√≥n**: Febrero 2026
